{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from dl4cv.utils.object_detect_utils import get_iou, fix_orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tacos(\n",
    "    dataset_path=\"/dtu/datasets1/02514/data_wastedetection\",\n",
    "    outpath=\"/dtu/blackhole/0f/160495/s210527/taco\",\n",
    "    splits=[1000, 250, 250],\n",
    "    seed=8008,\n",
    "):\n",
    "    \"\"\"Split the taco dataset into train, val, and test sets.\"\"\"\n",
    "    print(\"Splitting dataset into train, val, and test sets\")\n",
    "    anns_file_path = dataset_path + \"/\" + \"annotations.json\"\n",
    "    # Read annotations\n",
    "    with open(anns_file_path, \"r\") as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    # Split annotations\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(0, 1500, 1)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[: splits[0]]\n",
    "    val_indices = indices[splits[0] : splits[0] + splits[1]]\n",
    "    test_indices = indices[splits[0] + splits[1] :]\n",
    "\n",
    "    train_images = [dataset[\"images\"][i] for i in train_indices]\n",
    "    val_images = [dataset[\"images\"][i] for i in val_indices]\n",
    "    test_images = [dataset[\"images\"][i] for i in test_indices]\n",
    "\n",
    "    train_annotations = _get_annotations_for_images(\n",
    "        train_indices, dataset[\"annotations\"]\n",
    "    )\n",
    "    val_annotations = _get_annotations_for_images(val_indices, dataset[\"annotations\"])\n",
    "    test_annotations = _get_annotations_for_images(test_indices, dataset[\"annotations\"])\n",
    "\n",
    "    train_dataset = {\n",
    "        \"images\": train_images,\n",
    "        \"annotations\": train_annotations,\n",
    "        \"categories\": dataset[\"categories\"],\n",
    "    }\n",
    "    val_dataset = {\n",
    "        \"images\": val_images,\n",
    "        \"annotations\": val_annotations,\n",
    "        \"categories\": dataset[\"categories\"],\n",
    "    }\n",
    "    test_dataset = {\n",
    "        \"images\": test_images,\n",
    "        \"annotations\": test_annotations,\n",
    "        \"categories\": dataset[\"categories\"],\n",
    "    }\n",
    "\n",
    "    print(\"Train dataset size: \", len(train_dataset[\"images\"]))\n",
    "    print(\"Val dataset size: \", len(val_dataset[\"images\"]))\n",
    "    print(\"Test dataset size: \", len(test_dataset[\"images\"]))\n",
    "    print(f'Train annotations size: {len(train_dataset[\"annotations\"])}')\n",
    "    print(f'Val annotations size: {len(val_dataset[\"annotations\"])}')\n",
    "    print(f'Test annotations size: {len(test_dataset[\"annotations\"])}')\n",
    "\n",
    "    supercats = set([cat[\"supercategory\"] for cat in dataset[\"categories\"]])\n",
    "    cat2supercat = {cat[\"id\"]: cat[\"supercategory\"] for cat in dataset[\"categories\"]}\n",
    "    supercat2id = {sc: i for i, sc in enumerate(supercats)}\n",
    "    cat2supercat_encoded = {\n",
    "        cat_id: supercat2id[cat2supercat[cat_id]] for cat_id in cat2supercat\n",
    "    }\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, cat2supercat_encoded, supercat2id\n",
    "\n",
    "\n",
    "def _get_annotations_for_images(image_ids, annotations):\n",
    "    anns = [a for a in annotations if a[\"image_id\"] in image_ids]\n",
    "    return anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selective_search(\n",
    "    dataset,\n",
    "    catid_to_supercat_label_mapping,\n",
    "    outpath,\n",
    "    target_size=[1280, 720],\n",
    "    dataset_root_path=\"/dtu/datasets1/02514/data_wastedetection\",\n",
    "    train=False,\n",
    "):\n",
    "    BACKGROUND_LABEL = -1\n",
    "    label_map = catid_to_supercat_label_mapping\n",
    "\n",
    "    for orientation in ExifTags.TAGS.keys():\n",
    "        if ExifTags.TAGS[orientation] == \"Orientation\":\n",
    "            break\n",
    "\n",
    "    cv2.setUseOptimized(True)\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for image_index, i in tqdm(\n",
    "        enumerate(dataset[\"images\"]),\n",
    "        desc=\"running selective search\",\n",
    "        total=len(dataset[\"images\"]),\n",
    "    ):\n",
    "        annot_boxes = [\n",
    "            annotation[\"bbox\"]\n",
    "            for annotation in dataset[\"annotations\"]\n",
    "            if annotation[\"image_id\"] == i[\"id\"]\n",
    "        ]\n",
    "        annots_labels = [\n",
    "            label_map[annotation[\"category_id\"]]\n",
    "            for annotation in dataset[\"annotations\"]\n",
    "            if annotation[\"image_id\"] == i[\"id\"]\n",
    "        ]\n",
    "        filename = os.path.join(dataset_root_path, i[\"file_name\"])\n",
    "\n",
    "        try:\n",
    "            I = fix_orientation(filename, orientation)\n",
    "\n",
    "            image = cv2.cvtColor(np.array(I), cv2.COLOR_RGB2BGR)\n",
    "            print(f'Before resize {image.shape}')\n",
    "            scale_factor = (\n",
    "                image.shape[0] / target_size[0],\n",
    "                image.shape[1] / target_size[1],\n",
    "            )\n",
    "            image = cv2.resize(\n",
    "                image, (target_size[0], target_size[1]), interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "            print(f'After resize {image.shape}')\n",
    "\n",
    "            gtvalues = []\n",
    "            for a_index, annot in enumerate(annot_boxes):\n",
    "                x1 = int(annot[0] / scale_factor[0])\n",
    "                y1 = int(annot[1] / scale_factor[1])\n",
    "                x2 = int(annot[0] / scale_factor[0] + annot[2] / scale_factor[0])\n",
    "                y2 = int(annot[1] / scale_factor[1] + annot[3] / scale_factor[1])\n",
    "                gtvalues.append(\n",
    "                    {\n",
    "                        \"x1\": x1,\n",
    "                        \"x2\": x2,\n",
    "                        \"y1\": y1,\n",
    "                        \"y2\": y2,\n",
    "                        \"label\": annots_labels[a_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            ss.setBaseImage(image)  # setting given image as base image\n",
    "            ss.switchToSelectiveSearchFast()  # running selective search on bae image\n",
    "            ssresults = ss.process()  # processing to get the outputs\n",
    "\n",
    "            regions = {}\n",
    "            for e, result in enumerate(ssresults):\n",
    "                x, y, w, h = result\n",
    "                if y + h > image.shape[0]:\n",
    "                    print('DAFUQ')\n",
    "\n",
    "                result_coords = {\"x1\": x, \"x2\": x + w, \"y1\": y, \"y2\": y + h}\n",
    "                regions[e] = {\n",
    "                    \"coordinates\": result_coords,\n",
    "                    \"label\": BACKGROUND_LABEL,\n",
    "                    \"iou\": 0.0,\n",
    "                }\n",
    "                for gtval in gtvalues:\n",
    "                    iou = get_iou(\n",
    "                        gtval, result_coords\n",
    "                    )  # calculating IoU for each of the proposed regions\n",
    "                    if regions[e][\"iou\"] < iou and iou > 0.5:\n",
    "                        regions[e][\"iou\"] = iou\n",
    "                        regions[e][\"label\"] = gtval[\"label\"]\n",
    "            if train:\n",
    "                last_index = len(regions)\n",
    "                for indx, gtval in enumerate(gtvalues):\n",
    "                    regions[last_index + indx] = {\n",
    "                        \"coordinates\": gtval,\n",
    "                        \"label\": gtval[\"label\"],\n",
    "                        \"iou\": 1.0,\n",
    "                    }\n",
    "\n",
    "            # print(f'For Image {image_index} with filename {filename} got {len(regions)} regions')\n",
    "            # high_iou_regions = [region for region in regions.values() if region['iou'] > 0.5]\n",
    "            # low_iou_regions = [region for region in regions.values() if region['iou'] < 0.5]\n",
    "            # print(f'For Image {image_index} with filename {filename} got {len(high_iou_regions)} regions with iou > 0.5')\n",
    "            # print(high_iou_regions)\n",
    "            # print(f'For Image {image_index} with filename {filename} got {len(low_iou_regions)} regions with iou < 0.5')\n",
    "            # print(low_iou_regions[:5])\n",
    "            out[image_index] = {\n",
    "                \"regions\": regions,\n",
    "                \"filename\": filename,\n",
    "                \"image_id\": i[\"id\"],\n",
    "            }\n",
    "\n",
    "        except Exception as excpt:\n",
    "            print(excpt)\n",
    "            print(\"error in \" + filename)\n",
    "            continue\n",
    "        if image_index >= 1:\n",
    "            return out, image\n",
    "    \n",
    "    outpath = Path(outpath)\n",
    "    outfile_path = outpath / f\"ss_regions_{outpath.parts[-1]}.pkl\"\n",
    "\n",
    "    with open(str(outfile_path), \"wb\") as fp:\n",
    "        pickle.dump(out, fp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, val, and test sets\n",
      "Train dataset size:  1000\n",
      "Val dataset size:  250\n",
      "Test dataset size:  250\n",
      "Train annotations size: 3101\n",
      "Val annotations size: 934\n",
      "Test annotations size: 749\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset, cat_mapping, supercat2id = split_tacos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 16,\n",
       " 1: 18,\n",
       " 2: 24,\n",
       " 3: 24,\n",
       " 4: 4,\n",
       " 5: 4,\n",
       " 6: 4,\n",
       " 7: 17,\n",
       " 8: 17,\n",
       " 9: 14,\n",
       " 10: 6,\n",
       " 11: 6,\n",
       " 12: 6,\n",
       " 13: 15,\n",
       " 14: 15,\n",
       " 15: 15,\n",
       " 16: 15,\n",
       " 17: 15,\n",
       " 18: 15,\n",
       " 19: 15,\n",
       " 20: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 23: 0,\n",
       " 24: 0,\n",
       " 25: 9,\n",
       " 26: 2,\n",
       " 27: 26,\n",
       " 28: 26,\n",
       " 29: 27,\n",
       " 30: 21,\n",
       " 31: 21,\n",
       " 32: 21,\n",
       " 33: 21,\n",
       " 34: 3,\n",
       " 35: 3,\n",
       " 36: 1,\n",
       " 37: 1,\n",
       " 38: 1,\n",
       " 39: 1,\n",
       " 40: 1,\n",
       " 41: 1,\n",
       " 42: 1,\n",
       " 43: 20,\n",
       " 44: 20,\n",
       " 45: 20,\n",
       " 46: 20,\n",
       " 47: 20,\n",
       " 48: 12,\n",
       " 49: 13,\n",
       " 50: 10,\n",
       " 51: 8,\n",
       " 52: 25,\n",
       " 53: 23,\n",
       " 54: 11,\n",
       " 55: 19,\n",
       " 56: 19,\n",
       " 57: 5,\n",
       " 58: 22,\n",
       " 59: 7}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ss = run_selective_search(\n",
    "    train_dataset,\n",
    "    cat_mapping,\n",
    "    outpath=\"/dtu/blackhole/0f/160495/s210527/taco/train\",\n",
    "    train=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = train_ss[0]\n",
    "image = train_ss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "\n",
    "#PIL_image = Image.fromarray(image.astype('uint8'), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Show image\n",
    "fig,ax = plt.subplots(1)\n",
    "plt.axis('off')\n",
    "plt.imshow(PIL_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = regions[1]\n",
    "original_image = regions['filename']\n",
    "original_image = Image.open(original_image)\n",
    "# Show image\n",
    "fig,ax = plt.subplots(1)\n",
    "plt.axis('off')\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions['regions'].values():\n",
    "    if region['coordinates']['y1'] > 720 or region['coordinates']['y2'] > 720:\n",
    "        print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
