{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "from hydra import compose, initialize\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from dl4cv.utils.utils import set_seed\n",
    "from dl4cv.lightning_classes.plmodel import LitODModel\n",
    "from dl4cv.datasets import build_taco \n",
    "from dl4cv.datasets.taco_data import taco_val_test_collate_fn\n",
    "from dl4cv.utils.object_detect_utils import get_iou, fix_orientation\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"./configs\")\n",
    "inference_cfg = compose(config_name=\"config_taco_training\")\n",
    "inference_cfg[\"inference\"][\"run_name\"] = 'Resnet50_Whale_250x250'\n",
    "inference_cfg[\"inference\"][\"device\"] = 'cuda:1'\n",
    "path = f\"outputs/{inference_cfg.inference.run_name}/.hydra/config.yaml\"\n",
    "with open(path) as cfg:\n",
    "    cfg_yaml = yaml.safe_load(cfg)\n",
    "cfg_yaml[\"inference\"] = inference_cfg[\"inference\"]\n",
    "cfg_yaml[\"datamodule\"][\"test\"][\"params\"][\"num_to_return\"] = 100\n",
    "cfg = OmegaConf.create(cfg_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Run pytorch-lightning model inference\n",
    "    Args:\n",
    "        cfg: hydra config\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    set_seed(cfg.training.seed)\n",
    "\n",
    "    device = torch.device(cfg.inference.device)\n",
    "\n",
    "    model_names = glob.glob(f\"outputs/{cfg.inference.run_name}/saved_models/*.ckpt\")\n",
    "    _, _, test_set = build_taco(cfg)\n",
    "    test_set.num_to_return = 25\n",
    "    # Dirty trick to get the ground truth boxes\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        collate_fn=taco_val_test_collate_fn,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    lit_model = LitODModel.load_from_checkpoint(checkpoint_path=model_names[0], cfg=cfg)\n",
    "    lit_model.to(device)\n",
    "    predictions = []\n",
    "    for batch in tqdm(loader):\n",
    "        # move batch elements to device\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        predictions.append(\n",
    "            lit_model.nms_on_image(\n",
    "                batch,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:45<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "53\n",
      "80\n",
      "92\n",
      "106\n",
      "112\n",
      "118\n",
      "123\n",
      "125\n",
      "173\n",
      "177\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "# find an index in predictions where the kept_preds are not empty\n",
    "for i in range(len(predictions)):\n",
    "    if len(predictions[i]['kept_preds']) > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(pred, truth, iou_threshold = 0.5, num_classes = 29, per_class = False):\n",
    "    # compute mAP https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/metrics/mean_avg_precision.py\n",
    "    # `pred` is given in [[{'bbox':{'x1', 'x2', 'y1', 'y2'}, 'class'(int), 'conf'}, ...], ...]\n",
    "    # `truth` is given in [[{'x1', 'x2', 'y1', 'y2', 'class'(int)}, more boxes...], ...]\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(1, num_classes): # class '0' is background\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        total_true_bboxes = 0\n",
    "\n",
    "        # list detected(predicted) objects of class 'c'\n",
    "        detections = []\n",
    "\n",
    "        for idx, prs in enumerate(pred):\n",
    "          for pr in prs:\n",
    "            if pr['class'] == c:\n",
    "                detections.append((pr['conf'], idx, pr['bbox']))\n",
    "\n",
    "        # make checkbox for checking whether gt object was detected\n",
    "        total_true_bboxes = 0\n",
    "        is_detected = []\n",
    "        for gts in pred:\n",
    "          is_detected.append([False for _ in gts])\n",
    "          total_true_bboxes += sum([gt['class']==c for gt in gts])\n",
    "\n",
    "        detections.sort(reverse=True)\n",
    "\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            num_gts = len(truth[detection[1]])\n",
    "\n",
    "            # find most closest g.t box to pred as best_gt_idx\n",
    "            best_iou = 0\n",
    "            for idx, gt in enumerate(truth[detection[1]]):\n",
    "                #print(gt, detection[2])\n",
    "                \n",
    "                iou = get_iou(gt, detection[2])\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            # if considered found\n",
    "            #try:\n",
    "            #  print(best_iou, truth[detection[1]][best_gt_idx], detection[2])\n",
    "            #except:\n",
    "            #  pass\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if is_detected[detection[1]][best_gt_idx] == False:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    is_detected[detection[1]][best_gt_idx] = True\n",
    "                else: # duplicate is FP\n",
    "                    FP[detection_idx] = 1\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "        \n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        #if len(TP)>0 and len(FP)>0:\n",
    "        #  print(TP_cumsum[-1], FP_cumsum[-1])\n",
    "        #print(total_true_bboxes)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)  # ratio of detected objects!\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)         # ratio of predictions that are true objects!\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        #print(precisions, recalls, torch.trapz(precisions, recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "        #print('----------')\n",
    "    if per_class: \n",
    "        return average_precisions\n",
    "    else:\n",
    "        return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kept_preds': [{'bbox': {'x1': 24, 'y1': 42, 'x2': 233, 'y2': 202},\n",
       "   'conf': 0.24010595679283142,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 12,\n",
       "   'image_id': 823}],\n",
       " 'P': [{'bbox': {'x1': 16, 'y1': 41, 'x2': 224, 'y2': 162},\n",
       "   'conf': 0.18989835679531097,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 27, 'y1': 45, 'x2': 235, 'y2': 203},\n",
       "   'conf': 0.2335733324289322,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 12,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 34, 'y1': 54, 'x2': 224, 'y2': 152},\n",
       "   'conf': 0.25377675890922546,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 16, 'y1': 41, 'x2': 237, 'y2': 203},\n",
       "   'conf': 0.22333475947380066,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 12,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 29, 'y1': 45, 'x2': 227, 'y2': 162},\n",
       "   'conf': 0.23895218968391418,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 29, 'y1': 45, 'x2': 218, 'y2': 162},\n",
       "   'conf': 0.2231355607509613,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 31, 'y1': 53, 'x2': 228, 'y2': 186},\n",
       "   'conf': 0.23362396657466888,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 12,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 34, 'y1': 45, 'x2': 213, 'y2': 162},\n",
       "   'conf': 0.2279040366411209,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 16, 'y1': 41, 'x2': 224, 'y2': 152},\n",
       "   'conf': 0.2503047585487366,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 24, 'y1': 42, 'x2': 233, 'y2': 202},\n",
       "   'conf': 0.24010595679283142,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 12,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 34, 'y1': 45, 'x2': 213, 'y2': 161},\n",
       "   'conf': 0.21237628161907196,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 31, 'y1': 54, 'x2': 228, 'y2': 162},\n",
       "   'conf': 0.17196960747241974,\n",
       "   'pred_class': 18,\n",
       "   'true_class': 18,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 180, 'y1': 149, 'x2': 237, 'y2': 185},\n",
       "   'conf': 0.6878181099891663,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 178, 'y1': 146, 'x2': 214, 'y2': 158},\n",
       "   'conf': 0.750049352645874,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 96, 'y1': 235, 'x2': 157, 'y2': 250},\n",
       "   'conf': 0.7046212553977966,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 31, 'y1': 56, 'x2': 154, 'y2': 151},\n",
       "   'conf': 0.37474992871284485,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 142, 'y1': 81, 'x2': 175, 'y2': 108},\n",
       "   'conf': 0.673040509223938,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 181, 'y1': 53, 'x2': 204, 'y2': 67},\n",
       "   'conf': 0.5018054842948914,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 22, 'y1': 53, 'x2': 250, 'y2': 250},\n",
       "   'conf': 0.3725763261318207,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 96, 'y1': 235, 'x2': 153, 'y2': 250},\n",
       "   'conf': 0.8137900233268738,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 181, 'y1': 62, 'x2': 221, 'y2': 134},\n",
       "   'conf': 0.7600070834159851,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 144, 'y1': 84, 'x2': 174, 'y2': 107},\n",
       "   'conf': 0.5860962271690369,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 65, 'y1': 232, 'x2': 110, 'y2': 250},\n",
       "   'conf': 0.7612820267677307,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 27, 'y1': 164, 'x2': 122, 'y2': 203},\n",
       "   'conf': 0.6051526665687561,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823},\n",
       "  {'bbox': {'x1': 144, 'y1': 83, 'x2': 174, 'y2': 106},\n",
       "   'conf': 0.6571753025054932,\n",
       "   'pred_class': 28,\n",
       "   'true_class': 28,\n",
       "   'image_id': 823}],\n",
       " 'gt_regions': tensor([[ 32, 222,  53, 150],\n",
       "         [ 29, 234,  42, 202]], device='cuda:1'),\n",
       " 'gt_labels': tensor([18, 12], device='cuda:1')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
